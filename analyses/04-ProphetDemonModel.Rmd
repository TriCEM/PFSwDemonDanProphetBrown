---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Building a Prophet-Demon Prediction Model


## Prophet Model
The prophet model is a CNN connected to a fully connected FFNN. It takes as its input the image (adjacency) matrix and outputs a final size prediction ($s$). The `R-Tensorflow/Keras` implementation allows any number of batches to be fed into the model for training/prediction

```{r, eval = F, echo = T}
prophet <- ProphetFCNN(input_shape_dim = c(batch_size, N, N, 1))
```

## Demon Model
The demon model is a variational autoencoder ([Kingma and Welling 2013](https://arxiv.org/abs/1312.6114); [Kingma and Welling 2019](https://arxiv.org/abs/1906.02691)). Its input is an image (adjacency) matrix and encodes the input network in a lower dimensional latent space as a mean coding. A new coding is then sampled by adding Gaussian noise to the mean coding, which in turn is passed to the decoder to output a reconstructed image of an adjanceny matrix (i.e. network). Of note, we natively produced probabilities of connections but can use the Gumbell-Softmax trick to discretize these probabilties into binary connection [Jang et al. 2016](https://arxiv.org/abs/1611.01144). As a result, the demon autoencoder once trained can generate new network instances that resemble the networks that incorporate our previous simulation work (degree distribution heterogeneity, modularity/unity, clustering, dynamicity) as well as other networks native to `igraph`.
\br
As discussed in section 5, to date, the VAE does not capture the symmetry symmetry of the adjacency matrix (in the case of undirected links).  
\br
\br
A demon model can be called as follows: 
```{r, eval = F, echo = T}
demon <- DemonVariationalAutoEncoder(original_dim = N^2, intermediate_dim = 64, latent_dim = 32)
demon %>% compile(optimizer = keras::optimizer_adam(learning_rate = 1e-3),
                  loss = loss_mean_squared_error())

```


## Faust and the Prophet-Demon Dilemma
The overall goal of the Prophet-Demon Dilemma is to use the demon to better the prophet's predictions in the spirit of a Generative Adversial Network ([Goodfellow et al. 2014](https://arxiv.org/abs/1406.2661)). However, in order to "capture" the output of a new network generated by the demon VAE, we must run `fomes` to get a realized set of simulations that we can calculate final sizes from. This step introduces stochasticity into the predictions and the GAN-framework that erodes the gradients needed for gradient-descent/backpropagation in our CNN-FFNN. 
\br
To overcome this issue, we introduce a "faust" actor that uses Reinforcement Learning with a Deep Q-Network approach. Faust "struggles" between the Prophet and Demon and does his best to make sure both are properly tuned. This framework is analagous to a "see-saw" model where the DQN is attempting to balance the two seats. In this framework, our balancing equation is: 
$$
b = prophet_{weight} * P(x | s_{i:reps}) + demon_{weight} * \frac{1}{var (s_{i:reps})}
$$
where $P(x | s_{i:reps})$ is the probability of the Prophet's prediction given the empiric distribution of final sizes generated by `fomes` from the input contact network and $\frac{1}{var (s_{i:reps})}$. Notably, the variance is final distribution sizes is normalized so that it is on the same scale as the aforementioned probability distribution. 

### Pratical Approach
1. Train prophet on 80% of simulated data from above
  + Calculate initial validation score from prophet
2. Train demon on 100% of simulated data from above
3. Run faustRL for 100 steps (100 trainings of either model)
4. Reevaluate the Prophet's predicitons 
_Below, we show the results of this approach:_

```{r}


```
